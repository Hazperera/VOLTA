{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import graphAlgorithms as ga"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Pre-Process Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data set \n",
    "\n",
    "you can store your networks in any common format \n",
    "\n",
    "the graphAlgorithms package requires that the networks are provided as NetworkX Graph objects - refer to its documentation for detailed instructions\n",
    "\n",
    "the networks should be weighted, if you have an unweighted network then assign all edges the same edge weight\n",
    "\n",
    "the package assumes \"weight\" to be the default edge weight label, but this can be set when needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#location where the raw data files are stored, it is set to run from the installation folder\n",
    "#- if applicable please change or CHANGE to the location of your networks\n",
    "\n",
    "graph_location = \"../networks/edgelists/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#location where output should be saved\n",
    "#Please set location\n",
    "location = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below an example on how to load an Edgelist with column headings into a NetworkX Graph object\n",
    "\n",
    "There are multiple examples on how to load different formats in the import and export networks notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "labels = []\n",
    "networks_graphs = []\n",
    "\n",
    "print(\"load networks\")\n",
    "#gets all files located in the specified folder that end on .edgelist\n",
    "#CHANGE the ending if your files end differently\n",
    "for path in glob.glob(graph_location +\"*.edgelist\"):\n",
    "    \n",
    "    #you can specify that only part of the file name should be used as network name for later identification\n",
    "    name =  path.split(\"/\")[-1].replace(\".rds.edgelist\", \"\")\n",
    "    \n",
    "    \n",
    "    #read the edgelist file as a dataframe\n",
    "    fh = pd.read_csv(path, sep=\"\\t\")\n",
    "    #convert it into a NetworkX graph G and specify the column names of the node pairs\n",
    "    G=nx.from_pandas_edgelist(fh, \"V1\", \"V2\")\n",
    "    \n",
    "    #if you have an unweighted network assign all edges the same edge weight - here a value of 1 is assigned\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        d['weight'] = 1\n",
    "        \n",
    "    \n",
    "    #save the graph objects to a list (only suitable if small networks are processed)\n",
    "    #this is the main objects used for the examples below, which contains all networks\n",
    "    networks_graphs.append(G)\n",
    "    labels.append(name)\n",
    "   \n",
    "\n",
    "    \n",
    "\n",
    "    print(\"loaded\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the graph objects are converted into a list of lists format, which is used by most of the distance functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = ga.get_node_similarity.preprocess_graph(networks_graphs, attribute=\"weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the networks are now in this format\n",
    "\n",
    "in each sublists item 0 & 1 are node ids and the edge weight is at position 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL: Convert the node names to ints\n",
    "\n",
    "make sure you save the mapping file to retrieve your original node ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_lists, mapping = ga.get_node_similarity.preprocess_node_list(networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(location + \"node_id_mapping.pckl\", \"wb\") as f:\n",
    "    pickle.dump(mapping, f, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute node properties such as degree centrality, betweenness centrality and closeness centrality and shared nodes between networks (when you have networks with different nodes) \n",
    "\n",
    "sorted_nodes contains the node ids sorted after the selected properties as well as the mean and median ranking\n",
    "\n",
    "centrality_values contains the selected properties for each node\n",
    "\n",
    "shared_nodes contains for each nodes in how many networks it occures (this may be useful for networks that do not have the same nodes)\n",
    "\n",
    "binary contains the shared nodes in a binary representation\n",
    "\n",
    "you can select which format is the most suitable for you analysis\n",
    "\n",
    "make sure to remove asynchrone option when running in jupyter notebooks\n",
    "\n",
    "this is a wrapper function - if you are only interested in one output you can call the single function directly, for this please refer to the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted_nodes, shared_nodes, binary, centrality_values = ga.get_node_similarity.sort_list_and_get_shared(network_lists, mapping, networks_graphs, labels, degree_centrality=True, closeness_centrality=True, betweenness=True, degree=False, in_async=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO maybe use kendall instead of wrapper since others are not used/ applicable in this case??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on this data now multiple distances between the networks can be calculated\n",
    "\n",
    "this is a wrapper function calling different distance measures, if you are interested in only one distance you can use the single function directly. For this refer to the documentation\n",
    "\n",
    "this wrapper returns distance/similarity matrices of jaccard similarity, jaccard distance, percentage of shared nodes, kendall rank correlation of degree centrality, closeness centrality and betweenness centrality and mean / median ranking as well as the corresponding p-values for the top and bottom kendall_x nodes (or as here all nodes), hamming distance and SMC similarity\n",
    "\n",
    "if not all values are needed to be calculated you can use the individual functions called in the wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j, jd, percentage, kendall_dc_top, b_dc_top, kendall_cc_top, b_cc_top, kendall_betweenness_top, b_b_top, kendall_avg_top, b_avg_top, hamming, kendall_dc_bottom , b_dc_bottom , kendall_cc_bottom , b_cc_bottom , kendall_betweenness_bottom , b_b_bottom , kendall_avg_bottom , b_avg_bottom , smc, kendall_med_top, b_med_top, kendall_med_bottom, b_med_bottom = ga.get_node_similarity.estimate_similarities_nodes(network_lists, sorted_nodes, binary,  kendall_x=len(mapping), is_file=False, in_async=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This distances can be merged into a single distance matrix or used individually\n",
    "\n",
    "Since all networks have the same nodes, we will only use the average rank correlation matrix but transform it into a distance - the correlation value c is transformed to a distance with (1-c)/2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_dist = kendall_med_top.copy()\n",
    "\n",
    "for index, x in np.ndenumerate(median_dist):\n",
    "    d = (1-x)/2\n",
    "    \n",
    "    median_dist[index[0]][index[1]] = d\n",
    "    \n",
    "    if index[0] == index[1]:\n",
    "        median_dist[index[0]][index[1]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_dist_nodes = median_dist.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))  \n",
    "\n",
    "sns.heatmap(median_dist, annot=False, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will use the same preprocessed networks as already generated\n",
    "\n",
    "since the here used networks are unweighted we can assigne each edge its edge betweenness value as weight and compare the networks based on this value for shared edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sort edges after edge betweenness\")\n",
    "bet = []\n",
    "graphs_with_betweenness = []\n",
    "for net in networks_graphs:\n",
    "    edges_betweenness = nx.edge_betweenness_centrality(net)\n",
    "    bet.append(edges_betweenness)\n",
    "    #write as new attribute to graph\n",
    "    temp = nx.set_edge_attributes(net, edges_betweenness, \"betweenness\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need to convert the networks again into the list of lists format, since this time the betweeness values will be used and assign each edge an id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks = ga.get_edge_similarity.preprocess_graph(networks_graphs, attribute=\"betweenness\")\n",
    "\n",
    "print(\"map edges to id\")\n",
    "\n",
    "network_lists, mapping = ga.get_edge_similarity.preprocess_edge_list(networks)\n",
    "\n",
    "with open(location + \"edge_id_mapping.pckl\", \"wb\") as f:\n",
    "    pickle.dump(mapping, f, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sort edges after betweenness value & estimate for each edge in which network it appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_networks, shared_edges, binary = ga.get_edge_similarity.sort_list_and_get_shared(networks, mapping, network_lists, labels, in_async=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute distances/ similarities between the networks based on their similarity in edges\n",
    "\n",
    "the wrapper function returns jaccard similarity, jaccard distance, kendall rank coefficient for kendall_x top and bottom edges (here ranked after betweenness), hamming distance and SMC similarity\n",
    "\n",
    "if only specific distances are needed the individual functions can be called, please refer to the documentation for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j, jd, percentage, kendall_top,b_top, kendall_bottom, b_bottom, hamming, smc = ga.get_edge_similarity.estimate_similarities_edges(network_lists, sorted_networks, binary,  kendall_x=100, is_file=False, in_async=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert the similairty measures into distances where applicable\n",
    "\n",
    "since the networks have different edges, we will use all metrics to create a combined distance matrix (if your network has partially different nodes, you can use these commands to merge & convert them as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "smc_dist = smc.copy()\n",
    "\n",
    "for index, x in np.ndenumerate(smc_dist):\n",
    "    d = 1-x\n",
    "    \n",
    "    smc_dist[index[0]][index[1]] = d\n",
    "    \n",
    "    if index[0] == index[1]:\n",
    "        smc_dist[index[0]][index[1]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_dist = percentage.copy()\n",
    "\n",
    "for index, x in np.ndenumerate(p_dist):\n",
    "    d = 1-x\n",
    "    \n",
    "    p_dist[index[0]][index[1]] = d\n",
    "    \n",
    "    if index[0] == index[1]:\n",
    "        p_dist[index[0]][index[1]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_dist = kendall_top.copy()\n",
    "\n",
    "for index, x in np.ndenumerate(k_dist):\n",
    "    d = (1-x)/2\n",
    "    \n",
    "    k_dist[index[0]][index[1]] = d\n",
    "    \n",
    "    if index[0] == index[1]:\n",
    "        k_dist[index[0]][index[1]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL merge all metrics into a median distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_dist = ga.clustering.create_median_distance_matrix([k_dist, jd, hamming, p_dist, smc_dist], set_diagonal = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_dist_edges = median_dist.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))  \n",
    "\n",
    "sns.heatmap(median_dist, annot=False, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structural Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we compute networ distances based on the graph structure alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is a wrapper function that computes a vector based on a few structural parameters implemented\n",
    "\n",
    "you can create your own vector by creating your own wrapper function or call the corresponding functions\n",
    "\n",
    "\n",
    "the vector will contain data about the number of nodes & edges, network density, amount of missing edges, cycles, shortest path distributions, clustering coefficient, degree centrality/ closeness centrality and betweenness centrality distribution\n",
    "\n",
    "\n",
    "some paramters can be expensive on large networks\n",
    "\n",
    "as input a list of NetworkX graph objects has to be provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectors = ga.get_network_structural_vector.estimate_vector(networks_graphs, edge_attribute=\"weight\", is_file=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on these vectors a distance matrix between the networks can be estimated\n",
    "\n",
    "there is a wrapper function which estimates the euclidean, canberra, correlation, cosine and jaccard distance based on the vectors (here distance metrics that are not in [0,1] are normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "euclidean, canberra, correlation, cosine, jaccard = ga.get_network_structural_vector.matrix_from_vector(vectors, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL: merge the distances into a single distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_dist = ga.clustering.create_median_distance_matrix([jaccard, euclidean, canberra, cosine], set_diagonal = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_dist_structural = median_dist.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))  \n",
    "\n",
    "sns.heatmap(median_dist, annot=False, ax=ax, xticklabels=labels, yticklabels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Walks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method aims at characterizing the structural/ connectivity similarities around a specific node in different networks.\n",
    "Are the same nodes connected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform random walks based on differen starting nodes. Later walks between networks for the same starting node will be compared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The here used networks have all the same nodes, so nodes can simply be set to the node object of one of the graphs.\n",
    "\n",
    "E.g. like this:\n",
    "nodes = networks_graphs[0].nodes()\n",
    "\n",
    "If the networks have different nodes then you can select which nodes should be compared. E.g. the union of nodes or only their intersection? Or to reduce computational power you can investigate only a subset of pre-selected nodes of interest.\n",
    "\n",
    "The example below uses the union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = []\n",
    "for net in networks_graphs:\n",
    "    for node in net.nodes():\n",
    "        if node not in nodes:\n",
    "            nodes.append(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each node in nodes 3 times node degree random walks of length 10 are performed. Edges are selected probabilistc based on their edge weight, which is assumed to be a similarity. If no edge weights are existing then all are considered equal. Since the networks used here have all the same edge weight, probabilistic is set to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performed_walks = ga.get_walk_distances.helper_walks(networks_graphs, nodes, labels, steps=10, number_of_walks=3, degree=True, probabilistic=False, weight =\"weight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are estimating for each starting node how often surrounding nodes/ edges have been visit w.r.t. all the visited nodes/ edges. Depending on your network sizes and selected nodes this can be quite memory intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_counts, edge_counts, nodes_frc, edges_frc = ga.get_walk_distances.helper_get_counts(labels, networks_graphs, performed_walks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to estimate network similarities based on the visited nodes. For each network pair, kendall rank correlation is calculated (of the top 50 nodes)  for the same starting node. The mean correlation value of all same node pairs for a network pair is estimated and returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_edges, results_nodes, results_edges_p, results_nodes_p = ga.get_walk_distances.helper_walk_sim(networks_graphs, performed_walks, nodes, labels, top=50, undirected=False, return_all = False, nodes_ranked=nodes_frc, edges_ranked=edges_frc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL: convert the correlation into a distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cor_nodes = results_nodes.copy()\n",
    "\n",
    "for index, x in np.ndenumerate(cor_nodes):\n",
    "    d = (1-x)/2\n",
    "    \n",
    "    cor_nodes[index[0]][index[1]] = d\n",
    "    \n",
    "    if index[0] == index[1]:\n",
    "        cor_nodes[index[0]][index[1]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))  \n",
    "\n",
    "sns.heatmap(cor_nodes, annot=False, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_edges = results_edges.copy()\n",
    "\n",
    "for index, x in np.ndenumerate(cor_edges):\n",
    "    d = (1-x)/2\n",
    "    \n",
    "    cor_edges[index[0]][index[1]] = d\n",
    "    \n",
    "    if index[0] == index[1]:\n",
    "        cor_edges[index[0]][index[1]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,8))  \n",
    "\n",
    "sns.heatmap(cor_edges, annot=False, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_dist = ga.clustering.create_median_distance_matrix([cor_nodes, cor_edges], set_diagonal = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_dist_walks = median_dist.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on each of the three median distance matrices, clustering algorithms are run (here we use three algorithms, but this can be adjusted to any algorithms that you prefer for your type of analysis)\n",
    "\n",
    "based on the individual clusterings a consensus clustering is created\n",
    "\n",
    "each clustering algorithm is tuned based on a individually modifiable multiobjective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = [median_dist_nodes, median_dist_edges, median_dist_structural, median_dist_walks]\n",
    "distances_name = [\"nodes\", \"edges\", \"structural\", \"walks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterings = {}\n",
    "\n",
    "for n in distances_name:\n",
    "    clusterings[n] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering is run and best k value is estimated on a multiobjective function, focusing on maximizing distance between clusters, minimizing distance within a cluster as well as having an even cluster size distribution\n",
    "For the best k value the algorithm is run 10 times (One of the 3 algorithms has some randomness. In order to not bias towards one algorithm all are run the same amount of time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "for i in range(2,len(labels)):\n",
    "    t.append(i)\n",
    "    \n",
    "hierarchical = {}\n",
    "for d in range(len(distances)):\n",
    "    dist = distances[d]\n",
    "    n = distances_name[d]\n",
    "    maxs = 10000\n",
    "\n",
    "    for i, k in enumerate(t):\n",
    "    #for i, k in enumerate([2, 3]):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        cl_labels = ga.clustering.hierarchical_clustering(dist, n_clusters=k, linkage=\"complete\")\n",
    "\n",
    "\n",
    "        #print(\"obj 1\")\n",
    "        avg_score = ga.clustering.multiobjective(dist, cl_labels, min_number_clusters=None, max_number_clusters=None, min_cluster_size = None, max_cluster_size=None, local =True, bet=False, e=None, s=None, cluster_size_distribution = True)\n",
    "\n",
    "\n",
    "\n",
    "        if avg_score < maxs:\n",
    "            maxs = avg_score\n",
    "            mk = k\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    hierarchical[n] = k\n",
    "    print(maxs, mk)\n",
    "    \n",
    "    print(\"creating clusterings for\", n, \"with k \", mk)\n",
    "    \n",
    "    for xx in range(10):\n",
    "    \n",
    "    \n",
    "        cl_labels = ga.clustering.hierarchical_clustering(dist, n_clusters=mk, linkage=\"complete\")\n",
    "        clusterings.setdefault(n, []).append(cl_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affinity propagation has no parameters to be set so does not need to be tuned\n",
    "\n",
    "it is also run 10x (or 10x the same clustering is appended) so that for the later consensus for each clustering algorithm the same number of clusterings are provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(len(distances)):\n",
    "    dist = distances[d]\n",
    "    n = distances_name[d]\n",
    "    \n",
    "    for xx in range(10):\n",
    "        cl_labels = ga.clustering.affinityPropagation_clustering(dist)\n",
    "        clusterings.setdefault(n, []).append(cl_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Mediods is tuned on the same multiobjective function and for the best k the algorithm is run 3 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "for i in range(2,len(labels)):\n",
    "    t.append(i)\n",
    "    \n",
    "kmed = {}\n",
    "for d in range(len(distances)):\n",
    "    dist = distances[d]\n",
    "    n = distances_name[d]\n",
    "    maxs = 10000\n",
    "\n",
    "    for i, k in enumerate(t):\n",
    "    #for i, k in enumerate([2, 3]):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        cl_labels, mediods = ga.clustering.kmedoids_clustering(dist, n_clusters=k)\n",
    "\n",
    "\n",
    "        #print(\"obj 1\")\n",
    "        avg_score = ga.clustering.multiobjective(dist, cl_labels, min_number_clusters=None, max_number_clusters=None, min_cluster_size = None, max_cluster_size=None, local =True, bet=False, e=None, s=None, cluster_size_distribution = True)\n",
    "\n",
    "\n",
    "\n",
    "        if avg_score < maxs:\n",
    "            maxs = avg_score\n",
    "            mk = k\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    kmed[n] = k\n",
    "    print(maxs, mk)\n",
    "    \n",
    "    print(\"creating clusterings for\", n, \"with k \", mk)\n",
    "    \n",
    "    for xx in range(10):\n",
    "    \n",
    "    \n",
    "        cl_labels, mediods = ga.clustering.kmedoids_clustering(dist, n_clusters=mk)\n",
    "        clusterings.setdefault(n, []).append(np.array(cl_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consensus Clustering is created from the individually performed clusterings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_clusterings = []\n",
    "\n",
    "for key in clusterings.keys():\n",
    "    for c in clusterings[key]:\n",
    "        \n",
    "        merged_clusterings.append(c.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all clusterings an agreement graph is created.\n",
    "\n",
    "Refer to the documentation for other parameter selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "consensus = ga.clustering.consensus_clustering(merged_clusterings, seed=1234, threshold=\"matrix\", per_node=False, rep = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The consensus yields x clusters\n",
    "By tweaking the treshold or using another method it is possible to tune the consensus based on your needs (you can also use the multiobjective function again to evaluate the consensus numerically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(consensus)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(labels, consensus)), \n",
    "               columns =['CHEMICAL', 'CLUSTER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "save for later examples\n",
    "\n",
    "TODO upload to git + all other intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(location+\"clustering_networks.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
