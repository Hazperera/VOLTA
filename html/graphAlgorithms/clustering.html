<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.5" />
<title>graphAlgorithms.clustering API documentation</title>
<meta name="description" content="Clustering algorithms that take as input a distance matrix." />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>graphAlgorithms.clustering</code></h1>
</header>
<section id="section-intro">
<p>Clustering algorithms that take as input a distance matrix.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;
Clustering algorithms that take as input a distance matrix.
&#39;&#39;&#39;
import pandas as pd
import glob
import sys
import os
import datetime
import math
import networkx as nx
import collections
#import matplotlib.pyplot as plt
import random
#import treelib as bt
import pickle
import itertools
from scipy.stats import kurtosis, skew, kendalltau
import statistics
import numpy as np
from collections import Counter
import markov_clustering as mc
import pickle
from netneurotools import cluster
from netneurotools import plotting
from sklearn.cluster import AgglomerativeClustering
import sklearn
from pyclustering.cluster.optics import optics, ordering_analyser, ordering_visualizer
import pyclustering
from random import randrange
from pyclustering.cluster.kmedoids import kmedoids
import itertools
#sys.path.insert(1, &#39;distances/&#39;)
import graphAlgorithms.simplification as simplification
import bct
import community as community_louvain
from sklearn.utils.validation import check_random_state



def consensus_clustering(clusterings, seed=123, threshold=0.75, per_node=True, rep=10):
    &#34;&#34;&#34;
    Finds a consensus clustering based on multiple provided clusterings.
    Based on netneurotools &amp; bct for whole graph thresholds   https://github.com/netneurolab/netneurotools , https://github.com/aestrivex/bctpy.
    Builds an agreement graph from different clusterings, removes weak edges &amp; performs community detection rep times (louvain) until convergance.
    Multiple options on how to identify weak edges are provided.

    Parameters:
        clusterings (list): list of arrays containing numeric class labels
        seed (int): seed to be used for random processes
        treshold (float or str): if float needs to be in [0,1]. If float and not per_node all edges with a weight lower than the treshold are removed
            in the agreement graph. If float and per_node is True then for each node the weakes treshold % of edges are removed, if they are weak for 
                both nodes making up the edge. If treshold is &#34;matrix&#34; then the teshold is estimated based on a permutation of the clusterings as implemented in netneurotools.
        per_node (boolean): if True treshold is applied on a per node pasis else it is applied gloablly.
        rep (int): how often louvain clustering is reapeated on the agreement graph.

    Returns:
        consensus class labels (array): of clustering
      
    
    &#34;&#34;&#34;
    clusterings = np.column_stack(clusterings)

    rs = check_random_state(seed)
    samp, comm = clusterings.shape


    agreement = bct.clustering.agreement(clusterings, buffsz=samp) / comm
    
    null_assign = np.column_stack([rs.permutation(i) for i in clusterings.T])
    null_agree = bct.clustering.agreement(null_assign, buffsz=samp) / comm
    
    if threshold is None:
        threshold = np.mean(null_agree)
        print(threshold)
        
    if threshold == &#34;matrix&#34;:
        #do on a per node basis
        print(&#34;matrix&#34;)
        cons = __get_consensus_node__(agreement, null_agree, rs, seed=seed, rep = rep)
        
            
        cons = np.array(cons)[0]
                
    else:
        if per_node:
            cons = __get_consensus_node_threshold__(agreement, node_threshold=threshold, seed=seed ,rep = rep)
            cons = np.array(cons)[0]
        else:
    
            consensus = bct.clustering.consensus_und(agreement, threshold, rep)


            cons = consensus.astype(int)
    
    return cons


def __get_consensus_node_threshold__(agreement, node_threshold=0.75, seed=123 ,rep = 10):

    &#34;&#34;&#34;
    called by consensus_clustering in a per node case

    for explanation and parameters refer to consensus_clustering
    &#34;&#34;&#34;
    
    #node treshold is mean of all connecting edges of node_treshold matrix
    
    #create treshold graph and node treshold
    
    #build graph from agreement matrix
    T = nx.from_numpy_matrix(agreement)
    #remove edges for each node that are below threshold %
    T = simplification.remove_edges_per_node(T, treshold=None, percentage=node_threshold, direction=&#34;bottom&#34;, attribute=&#34;weight&#34;)
        
    #run weighted louvain on T rep times
    clusterings = []
    for x in range(rep):
        partion = community_louvain.best_partition(T, weight=&#34;weight&#34;)
        clusterings.append(list(partion.values()))
        
    T = None
    #while clusterings are different re-run
    ct = np.zeros(agreement.shape)
    
    matrices = ct.copy()
    for c in clusterings:
        cm = ct.copy()
        
        for index in itertools.combinations(list(partion.keys()), 2):
            i0 = index[0]
            i1 = index[1]
            
            if c[i0] == c[i1]:
                cm[i0][i1] = 1
                cm[i1][i0] = 1
            else:
                cm[i0][i1] = 0
                cm[i1][i0] = 0
                
                
        matrices = matrices + cm
        
    matrices = matrices / len(clusterings)
    #if all clusterings are the same matrices should only have 0 and 1 values 
    #- set 1 values to 0 and check if vlaues &gt; 0 are left
    ind = matrices == 1
    matrices[ind] = 0
    
    rerun = np.any(matrices[:, 0] &gt; 0 )
    
    while rerun:
        print(&#34;rerun&#34;)
        clusterings = np.column_stack(clusterings)
        samp, comm = clusterings.shape

        agreement = bct.clustering.agreement(clusterings, buffsz=samp) / comm

        T = nx.from_numpy_matrix(agreement)
        #remove edges for each node that are below threshold %
        T = simplification.remove_edges_per_node(T, treshold=None, percentage=node_threshold, direction=&#34;bottom&#34;, attribute=&#34;weight&#34;)

        #run weighted louvain on T rep times
        clusterings = []
        for x in range(rep):
            partion = community_louvain.best_partition(T, weight=&#34;weight&#34;)
            clusterings.append(list(partion.values()))

        T = None
        #while clusterings are different re-run
        ct = np.zeros(agreement.shape)

        matrices = ct.copy()
        for c in clusterings:
            cm = ct.copy()

            for index in itertools.combinations(list(partion.keys()), 2):
                i0 = index[0]
                i1 = index[1]

                if c[i0] == c[i1]:
                    cm[i0][i1] = 1
                    cm[i1][i0] = 1
                else:
                    cm[i0][i1] = 0
                    cm[i1][i0] = 0


            matrices = matrices + cm

        matrices = matrices / len(clusterings)
        #if all clusterings are the same matrices should only have 0 and 1 values 
        #- set 1 values to 0 and check if vlaues &gt; 0 are left
        ind = matrices == 1
        matrices[ind] = 0
        
        rerun = np.any(matrices[:, 0] &gt; 0 )
 
        
    
    #print(clusterings)
    #add one
   
    return clusterings
        
    
        
def __get_consensus_node__(agreement, node_threshold, rs, seed=123 ,rep = 10):

    &#34;&#34;&#34;
    called by consensus_clustering in a per node case

    for explanation and parameters refer to consensus_clustering
    &#34;&#34;&#34;
    
    #node treshold is mean of all connecting edges of node_treshold matrix
    
    #create treshold graph and node treshold
    T = nx.from_numpy_matrix(node_threshold)
    
    t = {}
    for i in range(len(node_threshold)):
        w = []
        for e in T.edges(i):
            w.append(T[e[0]][e[1]][&#34;weight&#34;])
        t[i] = statistics.mean(w)
        
    #build graph from agreement matrix
    T = nx.from_numpy_matrix(agreement)
    #remove edges for each node that are below threshold
    for node in t.keys():
        th = t[node]
        
        d = []
        #get all edges and select ones to be deleted
        for e in T.edges(node):
            if T[e[0]][e[1]][&#34;weight&#34;] &lt; th:
                d.append(e)
                
        #remove edges
        T.remove_edges_from(d)
        
    #run weighted louvain on T rep times
    clusterings = []
    for x in range(rep):
        partion = community_louvain.best_partition(T, weight=&#34;weight&#34;)
        clusterings.append(list(partion.values()))
        
    T = None
    #while clusterings are different re-run
    ct = np.zeros(agreement.shape)
    
    matrices = ct.copy()
    for c in clusterings:
        cm = ct.copy()
        
        for index in itertools.combinations(list(partion.keys()), 2):
            i0 = index[0]
            i1 = index[1]
            
            if c[i0] == c[i1]:
                cm[i0][i1] = 1
                cm[i1][i0] = 1
            else:
                cm[i0][i1] = 0
                cm[i1][i0] = 0
                
                
        matrices = matrices + cm
        
    matrices = matrices / len(clusterings)
    #if all clusterings are the same matrices should only have 0 and 1 values 
    #- set 1 values to 0 and check if vlaues &gt; 0 are left
    ind = matrices == 1
    matrices[ind] = 0
    
    rerun = np.any(matrices[:, 0] &gt; 0 )
    
    while rerun:
        print(&#34;rerun&#34;)
        clusterings = np.column_stack(clusterings)
        samp, comm = clusterings.shape

        agreement = bct.clustering.agreement(clusterings, buffsz=samp) / comm

        null_assign = np.column_stack([rs.permutation(i) for i in clusterings.T])
        node_threshold = bct.clustering.agreement(null_assign, buffsz=samp) / comm
        
        T = nx.from_numpy_matrix(node_threshold)
    
        t = {}
        for i in range(len(node_threshold)):
            w = []
            for e in T.edges(i):
                w.append(T[e[0]][e[1]][&#34;weight&#34;])
            t[i] = statistics.mean(w)

        #build graph from agreement matrix
        T = nx.from_numpy_matrix(agreement)
        #remove edges for each node that are below threshold
        for node in t.keys():
            th = t[node]

            d = []
            #get all edges and select ones to be deleted
            for e in T.edges(node):
                if T[e[0]][e[1]][&#34;weight&#34;] &lt; th:
                    d.append(e)

            #remove edges
            T.remove_edges_from(d)

        #run weighted louvain on T rep times
        clusterings = []
        for x in range(rep):
            partion = community_louvain.best_partition(T, weight=&#34;weight&#34;)
            clusterings.append(list(partion.values()))

        T = None
        #while clusterings are different re-run
        ct = np.zeros(agreement.shape)

        matrices = ct.copy()
        for c in clusterings:
            cm = ct.copy()

            for index in itertools.combinations(list(partion.keys()), 2):
                i0 = index[0]
                i1 = index[1]

                if c[i0] == c[i1]:
                    cm[i0][i1] = 1
                    cm[i1][i0] = 1
                else:
                    cm[i0][i1] = 0
                    cm[i1][i0] = 0


            matrices = matrices + cm

        matrices = matrices / len(clusterings)
        #if all clusterings are the same matrices should only have 0 and 1 values 
        #- set 1 values to 0 and check if vlaues &gt; 0 are left
        ind = matrices == 1
        matrices[ind] = 0
        
        rerun = np.any(matrices[:, 0] &gt; 0 )
 
        
    
    #print(clusterings)
    #add one
   
    return clusterings
        
    
        

    
    


def multiobjective(X, labels, min_number_clusters=2, max_number_clusters=None, min_cluster_size = 10, max_cluster_size=None, local =True, bet=True, e=0.5, s=0.5, cluster_size_distribution = True):
    
    &#34;&#34;&#34;
        Multi objective function to evaluate clusterings. 
        
        Parameters:
            X (matrix): distance matrix, as used for clustering.
            labels (list): list of predicted labels. Needs to be in the same order as X.
            min_number_clusters (int or None): minimum number of allowed clusters, if less than a penalty is applied. If None will not be taken into account. 
            max_number_clusters (int or None): maximum number of clusters, if more then penalty is applied. If None will not be taken into account.
            min_cluster_size (int or None): for each cluster with less than x items a penalty is applied. If None will not be taken into account .
            max_cluster_size (int or None): for each cluster with more than x items a penalty is applied. If None will not be taken into account.
            local (boolean): if is True then objective aims at minimizing within cluster similarity based on thr data provided in X. If is False will be ignored.
            bet (boolean): if True objective aims at maximizing dissimilarity between clusters. If False will be ignored.
            e (float): in [0,1]. For each cluster with a mean similarity less than e an additional penalty is applied. If None will be ignored.
            s (float): in [0,1]. Between each cluster pair where thr distance is less than s an additional penalty is applied. If None will be ignored.
            cluster_size_distribution (boolean): if True mean difference of cluster size for each cluster to &#34;most equal&#34; partitioning is applied. Most equal partitioning is len(labels) / number of clusters. If False will be ignored.
            
        Returns:
            clustering score (float): the closer to 0 the better the clustering is with regards to the selected objectives.            
            
    &#34;&#34;&#34;
    
    
    obj = 0
    
    clus = Counter(labels)
    
    nr_clusters = max(labels)+1
    
    #transform labels into list of lists for each cluster one with index of items
    
    ll = []
    
    for r in range(nr_clusters):
        indices = [i for i, x in enumerate(labels) if x == r]
        ll.append(indices)
        
    #print(ll)
    if min_number_clusters is not None:
        if min_number_clusters &gt; nr_clusters:
            
            obj = obj + min_number_clusters/nr_clusters
            
            #print(&#34;min number of clusters penalty is&#34;,  min_number_clusters/nr_clusters)
            
    if max_number_clusters is not None:
        if max_number_clusters &gt; nr_clusters:
            obj = obj +  max_number_clusters/nr_clusters
            
            #print(&#34;max number of clusters penalty is&#34;, max_number_clusters/nr_clusters)
            
            
    if min_cluster_size is not None:
        t = sum(i &lt; min_cluster_size for i in list(clus.values()))
        obj = obj + (t/nr_clusters)
        
        #print(&#34;min_cluster_size penalty is&#34;, t/nr_clusters)
        
        
    if max_cluster_size is not None:
        t = sum(i &gt; max_cluster_size for i in list(clus.values()))
        obj = obj + (t/nr_clusters)
                  
        #print(&#34;max_cluster_size penalty is&#34;, t/nr_clusters)

    if cluster_size_distribution:
        m = len(labels) / nr_clusters
        t = []
        cl = Counter(labels)

        for i in cl.values():
            t.append(abs(i/m))

        t = statistics.mean(t)

        obj = obj + t


        
        
    if local:
        #get mean cluster similarity scores
        m = []
        pen = 0
        
        for c in ll:
            temp = []
            if len(c) &gt; 1:
                pairs = list(itertools.combinations(c, 2))

                for p in pairs:
                    temp.append(1-X[p[0]][p[1]])


            else:
                temp.append(0)
                
            #print(&#34;local&#34;, temp)
            if e is not None:
                if statistics.mean(temp) &lt; e:
                    pen = pen + 1
            m.append(statistics.mean(temp))

        if len(m) &gt; 1:
            obj = obj + (1-statistics.mean(m))
                  
            #print(&#34;local penalty is&#34;, (1-statistics.mean(m)))
        elif len(m) &gt; 0:
            obj = obj + 1-m[0]
                  
                  
            #print(&#34;local penalty is&#34;, 1-m[0])
            
        obj = obj + (pen/nr_clusters)
                  
                  
        
        
        
        
        
    if bet:
        #get mean dissimilarity between all clusters 
        #this is calculated on an item by item base
        
        m = []
        pen = 0
        
        for cc in list(itertools.combinations(range(nr_clusters),2)):
            c1 = ll[cc[0]]
            c2 = ll[cc[1]]
            
            temp = []
            for i1 in c1:
                for i2 in c2:
                    temp.append(X[i1][i2])
                    
            #print(&#34;bet&#34;, temp)
            if len(temp) &gt; 0:
                m.append(statistics.mean(temp))
                if s is not None:
                    if statistics.mean(temp) &lt; s:
                        pen = pen +1
                        
                        
            
            elif len(temp) &gt; 0:
                m.append(temp[0])
                
                if s is not None:
                    if temp[0] &lt; s:
                        pen = pen +1
                    
                    
        if len(m) &gt; 1:
            obj = obj + (1-statistics.mean(m))
                  
            #print(&#34;bet penalty is&#34;, 1-statistics.mean(m))
        elif len(m) &gt; 0:
            obj = obj + (1-m[0])
                  
            #print(&#34;bet penalty is&#34;, (1-m[0]))
            
            
        if s is not None:
            obj = obj + (pen/len(list(itertools.combinations(range(nr_clusters),2))))

            #print(&#34;bet penalty s is&#34;, (pen/len(list(itertools.combinations(range(nr_clusters),2)))))
        
        
        
        
        
    return obj
                
                
            
        
            
    
def create_mean_distance_matrix(matrices, set_diagonal = True):
    &#34;&#34;&#34;
    Creates a mean distance matrix out of individual distance matrices.

    Parameters:
        matrices (list): of numpy matrices. Items need to be in the same order in all the matrices. Matrices are assumed to be &#34;distance amtrices&#34;.
        set_diagonal (boolean): if True diagonal values are set to 0 automatically - this may be helpful if the distance measures applied do not return 0 distance for the same object.
        
    Returns:
        mean distance matrix (matrix): of input matrices
    &#34;&#34;&#34;

    if len(matrices) &lt; 2:
        print(&#34;matrices needs to contain at least two items in order to estimate its median&#34;)
        return None
    else:

        mean_dist = matrices[0].copy()

        for index, x in np.ndenumerate(mean_dist):
            i0 = index[0]
            i1 = index[1]
            
            temp = []
            for m in matrices:
                temp.append(m[i0][i1])

            d = statistics.mean(temp)
            
            mean_dist[index[0]][index[1]] = d
            
            if set_diagonal:
                if index[0] == index[1]:
                    mean_dist[index[0]][index[1]] = 0

        return mean_dist


def create_median_distance_matrix(matrices, set_diagonal = True):
    &#34;&#34;&#34;
    Creates a median distance matrix out of individual distance matrices.

    Parameters:
        matrices (list): of numpy matrices. Items need to be in the same order in all the matrices. Matrices are assumed to be &#34;distance amtrices&#34;.
        set_diagonal (boolean): if True diagonal values are set to 0 automatically - this may be helpful if the distance measures applied do not return 0 distance for the same object.
        
    Returns:
        median distance matrix (matrix): of input matrices
    &#34;&#34;&#34;
    if len(matrices) &lt; 2:
        print(&#34;matrices needs to contain at least two items in order to estimate its median&#34;)
        return None
    else:

        median_dist = matrices[0].copy()

        for index, x in np.ndenumerate(median_dist):
            i0 = index[0]
            i1 = index[1]
            
            temp = []
            for m in matrices:
                temp.append(m[i0][i1])

            d = statistics.median(temp)
            
            median_dist[index[0]][index[1]] = d
            
            if set_diagonal:
                if index[0] == index[1]:
                    median_dist[index[0]][index[1]] = 0

        return median_dist


def hierarchical_clustering(distance, n_clusters=2, linkage=&#34;complete&#34;):
    &#34;&#34;&#34;
    Hierarchical clustering of distance matrix. Based on sklearn.

    Parameters:
        distance (matrix): distance matrix to be used for clustering.
        n_clusters (int): number of to be computed clusters. Maximum allowed value is thr number of items in distance. Minimum allowed value is 2.
        linkage (str): linkage method to be used. Options are &#34;average&#34;, &#34;complete&#34; or &#34;single&#34;.
            
    Returns:
        labels (array): of clustering
    &#34;&#34;&#34;

    if n_clusters &lt; 2 or n_clusters &gt; len(distance):
        print(&#34;n_clusters is either too small or too large, n_clusters must be in range 2 &#34;, len(distance))
        return None

    elif linkage not in [&#34;average&#34;, &#34;complete&#34;, &#34;single&#34;]:
        print(&#34;linkage has to be average, complete or single&#34;)
        return None

    else:

        ag= AgglomerativeClustering(n_clusters = n_clusters, affinity=&#34;precomputed&#34;, distance_threshold=None, linkage=linkage).fit(distance)
        labels = ag.labels_

        return labels



def affinityPropagation_clustering(distance):
    &#34;&#34;&#34;
    Affinity propagation clustering on distance matrix. Based on sklearn

    Parameters:
        distance (matrix): distance matrix to be used for clustering.
        
    Returns:
        labels (array): of clustering
    &#34;&#34;&#34;

    

    db= sklearn.cluster.AffinityPropagation(affinity=&#34;precomputed&#34;).fit(distance)
    labels = db.labels_

    return labels

def __generate_empty__(x):
    &#34;&#34;&#34;
    generates empty dict to be used as input for other functions

    Input
        x int, lenght of keys to be generated

    Output
        dict
    &#34;&#34;&#34;

    empty = {}
    for i in range(x):
        empty[i] = 0

    return empty

def convert_clusters(clusters, v):

    &#34;&#34;&#34;
    Converts list of sublists into array of cluster labels,

    Parameters:
        clusters (list): list of sublists where each list contains indices of the items in this cluster and each sublist is a different cluster.
        v (list): of item IDs as contained in clusters.

    Returns:
        labels (array): of clustering
    &#34;&#34;&#34;
    d = {}
    for vv in v:
        d[vv] = 0

    empty = d.copy()
    for i in range(len(clusters)):
        for k in clusters[i]:
            empty[k] = i
            
    return list(empty.values())

def optics_clustering(distance, radius=2, neighbors=2, n_clusters=2):
    &#34;&#34;&#34;
    Optics clustering on a provided distance matrix. Based on pyclustering https://pyclustering.github.io/docs/0.8.2/html/de/d3b/classpyclustering_1_1cluster_1_1optics_1_1optics.html
    
    Parameters:
        distance (matrix): distance matrix to be used for clustering.
        radius (int): connectivity radius. 
        neighbors (int): in [1, number of samples -1]
        n_clusters (int): amount of clusters that should be found.

    Returns:
            labels (array): of clustering
    &#34;&#34;&#34;

    if n_clusters &lt; 2 or n_clusters &gt; len(distance):
        print(&#34;n_clusters is either too small or too large, n_clusters must be in range 2 &#34;, len(distance))
        return None

    elif neighbors &lt; 1 or neighbors &gt; len(distance)-1:
        print(&#34;neighbors has to be in range 1&#34;, len(distance)-1)
        return None

    else:

        optics_instance = optics(distance, radius, neighbors, n_clusters, data_type=&#34;distance_matrix&#34;)
        # Performs cluster analysis
        optics_instance.process()
        # Obtain results of clustering
        clusters = optics_instance.get_clusters()

        #converts output into labeled array
        empty = __generate_empty__(len(distance))
        

        labels = convert_clusters(clusters, empty)

        return labels



def kmedoids_clustering(distance, n_clusters=2):
    &#34;&#34;&#34;
    Kmediods clustering on a provided distance matrix. Based on pyclustering https://pyclustering.github.io/docs/0.8.2/html/de/d3b/classpyclustering_1_1cluster_1_1optics_1_1optics.html
    
    Parameters:
        distance (matrix): distance matrix to be used for clustering.
        n_clusters (int): amount of clusters that should be found.
        
    Returns:
        labels (array): of clustering
        created mediods (list): random created mediods used for clustering
    &#34;&#34;&#34;

    if n_clusters &lt; 2 or n_clusters &gt; len(distance):
        print(&#34;n_clusters is either too small or too large, n_clusters must be in range 2 &#34;, len(distance))
        return None

    

    else:

        #generate random mediods
        initial_medoids = []
    
        for ii in range(n_clusters):
            initial_medoids.append(randrange(len(distance)))

        kmedoids_instance = kmedoids(distance, initial_medoids, data_type=&#39;distance_matrix&#39;)
        # run cluster analysis and obtain results
        kmedoids_instance.process()
        clusters = kmedoids_instance.get_clusters()

        empty = __generate_empty__(len(distance))

        labels = convert_clusters(clusters, empty)


        return np.array(labels),  initial_medoids</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="graphAlgorithms.clustering.affinityPropagation_clustering"><code class="name flex">
<span>def <span class="ident">affinityPropagation_clustering</span></span>(<span>distance)</span>
</code></dt>
<dd>
<section class="desc"><p>Affinity propagation clustering on distance matrix. Based on sklearn</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>distance</code></strong> :&ensp;<code>matrix</code></dt>
<dd>distance matrix to be used for clustering.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>labels</code></strong> :&ensp;<code>array</code></dt>
<dd>of clustering</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def affinityPropagation_clustering(distance):
    &#34;&#34;&#34;
    Affinity propagation clustering on distance matrix. Based on sklearn

    Parameters:
        distance (matrix): distance matrix to be used for clustering.
        
    Returns:
        labels (array): of clustering
    &#34;&#34;&#34;

    

    db= sklearn.cluster.AffinityPropagation(affinity=&#34;precomputed&#34;).fit(distance)
    labels = db.labels_

    return labels</code></pre>
</details>
</dd>
<dt id="graphAlgorithms.clustering.consensus_clustering"><code class="name flex">
<span>def <span class="ident">consensus_clustering</span></span>(<span>clusterings, seed=123, threshold=0.75, per_node=True, rep=10)</span>
</code></dt>
<dd>
<section class="desc"><p>Finds a consensus clustering based on multiple provided clusterings.
Based on netneurotools &amp; bct for whole graph thresholds
<a href="https://github.com/netneurolab/netneurotools">https://github.com/netneurolab/netneurotools</a> , <a href="https://github.com/aestrivex/bctpy.">https://github.com/aestrivex/bctpy.</a>
Builds an agreement graph from different clusterings, removes weak edges &amp; performs community detection rep times (louvain) until convergance.
Multiple options on how to identify weak edges are provided.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>clusterings</code></strong> :&ensp;<code>list</code></dt>
<dd>list of arrays containing numeric class labels</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>seed to be used for random processes</dd>
<dt><strong><code>treshold</code></strong> :&ensp;<code>float</code> or <code>str</code></dt>
<dd>if float needs to be in [0,1]. If float and not per_node all edges with a weight lower than the treshold are removed
in the agreement graph. If float and per_node is True then for each node the weakes treshold % of edges are removed, if they are weak for
both nodes making up the edge. If treshold is "matrix" then the teshold is estimated based on a permutation of the clusterings as implemented in netneurotools.</dd>
<dt><strong><code>per_node</code></strong> :&ensp;<code>boolean</code></dt>
<dd>if True treshold is applied on a per node pasis else it is applied gloablly.</dd>
<dt><strong><code>rep</code></strong> :&ensp;<code>int</code></dt>
<dd>how often louvain clustering is reapeated on the agreement graph.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>consensus</code> <code>class</code> <code>labels</code> (<code>array</code>): of <code>clustering</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def consensus_clustering(clusterings, seed=123, threshold=0.75, per_node=True, rep=10):
    &#34;&#34;&#34;
    Finds a consensus clustering based on multiple provided clusterings.
    Based on netneurotools &amp; bct for whole graph thresholds   https://github.com/netneurolab/netneurotools , https://github.com/aestrivex/bctpy.
    Builds an agreement graph from different clusterings, removes weak edges &amp; performs community detection rep times (louvain) until convergance.
    Multiple options on how to identify weak edges are provided.

    Parameters:
        clusterings (list): list of arrays containing numeric class labels
        seed (int): seed to be used for random processes
        treshold (float or str): if float needs to be in [0,1]. If float and not per_node all edges with a weight lower than the treshold are removed
            in the agreement graph. If float and per_node is True then for each node the weakes treshold % of edges are removed, if they are weak for 
                both nodes making up the edge. If treshold is &#34;matrix&#34; then the teshold is estimated based on a permutation of the clusterings as implemented in netneurotools.
        per_node (boolean): if True treshold is applied on a per node pasis else it is applied gloablly.
        rep (int): how often louvain clustering is reapeated on the agreement graph.

    Returns:
        consensus class labels (array): of clustering
      
    
    &#34;&#34;&#34;
    clusterings = np.column_stack(clusterings)

    rs = check_random_state(seed)
    samp, comm = clusterings.shape


    agreement = bct.clustering.agreement(clusterings, buffsz=samp) / comm
    
    null_assign = np.column_stack([rs.permutation(i) for i in clusterings.T])
    null_agree = bct.clustering.agreement(null_assign, buffsz=samp) / comm
    
    if threshold is None:
        threshold = np.mean(null_agree)
        print(threshold)
        
    if threshold == &#34;matrix&#34;:
        #do on a per node basis
        print(&#34;matrix&#34;)
        cons = __get_consensus_node__(agreement, null_agree, rs, seed=seed, rep = rep)
        
            
        cons = np.array(cons)[0]
                
    else:
        if per_node:
            cons = __get_consensus_node_threshold__(agreement, node_threshold=threshold, seed=seed ,rep = rep)
            cons = np.array(cons)[0]
        else:
    
            consensus = bct.clustering.consensus_und(agreement, threshold, rep)


            cons = consensus.astype(int)
    
    return cons</code></pre>
</details>
</dd>
<dt id="graphAlgorithms.clustering.convert_clusters"><code class="name flex">
<span>def <span class="ident">convert_clusters</span></span>(<span>clusters, v)</span>
</code></dt>
<dd>
<section class="desc"><p>Converts list of sublists into array of cluster labels,</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>clusters</code></strong> :&ensp;<code>list</code></dt>
<dd>list of sublists where each list contains indices of the items in this cluster and each sublist is a different cluster.</dd>
<dt><strong><code>v</code></strong> :&ensp;<code>list</code></dt>
<dd>of item IDs as contained in clusters.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>labels</code></strong> :&ensp;<code>array</code></dt>
<dd>of clustering</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convert_clusters(clusters, v):

    &#34;&#34;&#34;
    Converts list of sublists into array of cluster labels,

    Parameters:
        clusters (list): list of sublists where each list contains indices of the items in this cluster and each sublist is a different cluster.
        v (list): of item IDs as contained in clusters.

    Returns:
        labels (array): of clustering
    &#34;&#34;&#34;
    d = {}
    for vv in v:
        d[vv] = 0

    empty = d.copy()
    for i in range(len(clusters)):
        for k in clusters[i]:
            empty[k] = i
            
    return list(empty.values())</code></pre>
</details>
</dd>
<dt id="graphAlgorithms.clustering.create_mean_distance_matrix"><code class="name flex">
<span>def <span class="ident">create_mean_distance_matrix</span></span>(<span>matrices, set_diagonal=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Creates a mean distance matrix out of individual distance matrices.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>matrices</code></strong> :&ensp;<code>list</code></dt>
<dd>of numpy matrices. Items need to be in the same order in all the matrices. Matrices are assumed to be "distance amtrices".</dd>
<dt><strong><code>set_diagonal</code></strong> :&ensp;<code>boolean</code></dt>
<dd>if True diagonal values are set to 0 automatically - this may be helpful if the distance measures applied do not return 0 distance for the same object.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>mean</code> <code>distance</code> <code>matrix</code> (<code>matrix</code>): of <code>input</code> <code>matrices</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_mean_distance_matrix(matrices, set_diagonal = True):
    &#34;&#34;&#34;
    Creates a mean distance matrix out of individual distance matrices.

    Parameters:
        matrices (list): of numpy matrices. Items need to be in the same order in all the matrices. Matrices are assumed to be &#34;distance amtrices&#34;.
        set_diagonal (boolean): if True diagonal values are set to 0 automatically - this may be helpful if the distance measures applied do not return 0 distance for the same object.
        
    Returns:
        mean distance matrix (matrix): of input matrices
    &#34;&#34;&#34;

    if len(matrices) &lt; 2:
        print(&#34;matrices needs to contain at least two items in order to estimate its median&#34;)
        return None
    else:

        mean_dist = matrices[0].copy()

        for index, x in np.ndenumerate(mean_dist):
            i0 = index[0]
            i1 = index[1]
            
            temp = []
            for m in matrices:
                temp.append(m[i0][i1])

            d = statistics.mean(temp)
            
            mean_dist[index[0]][index[1]] = d
            
            if set_diagonal:
                if index[0] == index[1]:
                    mean_dist[index[0]][index[1]] = 0

        return mean_dist</code></pre>
</details>
</dd>
<dt id="graphAlgorithms.clustering.create_median_distance_matrix"><code class="name flex">
<span>def <span class="ident">create_median_distance_matrix</span></span>(<span>matrices, set_diagonal=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Creates a median distance matrix out of individual distance matrices.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>matrices</code></strong> :&ensp;<code>list</code></dt>
<dd>of numpy matrices. Items need to be in the same order in all the matrices. Matrices are assumed to be "distance amtrices".</dd>
<dt><strong><code>set_diagonal</code></strong> :&ensp;<code>boolean</code></dt>
<dd>if True diagonal values are set to 0 automatically - this may be helpful if the distance measures applied do not return 0 distance for the same object.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>median</code> <code>distance</code> <code>matrix</code> (<code>matrix</code>): of <code>input</code> <code>matrices</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_median_distance_matrix(matrices, set_diagonal = True):
    &#34;&#34;&#34;
    Creates a median distance matrix out of individual distance matrices.

    Parameters:
        matrices (list): of numpy matrices. Items need to be in the same order in all the matrices. Matrices are assumed to be &#34;distance amtrices&#34;.
        set_diagonal (boolean): if True diagonal values are set to 0 automatically - this may be helpful if the distance measures applied do not return 0 distance for the same object.
        
    Returns:
        median distance matrix (matrix): of input matrices
    &#34;&#34;&#34;
    if len(matrices) &lt; 2:
        print(&#34;matrices needs to contain at least two items in order to estimate its median&#34;)
        return None
    else:

        median_dist = matrices[0].copy()

        for index, x in np.ndenumerate(median_dist):
            i0 = index[0]
            i1 = index[1]
            
            temp = []
            for m in matrices:
                temp.append(m[i0][i1])

            d = statistics.median(temp)
            
            median_dist[index[0]][index[1]] = d
            
            if set_diagonal:
                if index[0] == index[1]:
                    median_dist[index[0]][index[1]] = 0

        return median_dist</code></pre>
</details>
</dd>
<dt id="graphAlgorithms.clustering.hierarchical_clustering"><code class="name flex">
<span>def <span class="ident">hierarchical_clustering</span></span>(<span>distance, n_clusters=2, linkage='complete')</span>
</code></dt>
<dd>
<section class="desc"><p>Hierarchical clustering of distance matrix. Based on sklearn.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>distance</code></strong> :&ensp;<code>matrix</code></dt>
<dd>distance matrix to be used for clustering.</dd>
<dt><strong><code>n_clusters</code></strong> :&ensp;<code>int</code></dt>
<dd>number of to be computed clusters. Maximum allowed value is thr number of items in distance. Minimum allowed value is 2.</dd>
<dt><strong><code>linkage</code></strong> :&ensp;<code>str</code></dt>
<dd>linkage method to be used. Options are "average", "complete" or "single".</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>labels</code></strong> :&ensp;<code>array</code></dt>
<dd>of clustering</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def hierarchical_clustering(distance, n_clusters=2, linkage=&#34;complete&#34;):
    &#34;&#34;&#34;
    Hierarchical clustering of distance matrix. Based on sklearn.

    Parameters:
        distance (matrix): distance matrix to be used for clustering.
        n_clusters (int): number of to be computed clusters. Maximum allowed value is thr number of items in distance. Minimum allowed value is 2.
        linkage (str): linkage method to be used. Options are &#34;average&#34;, &#34;complete&#34; or &#34;single&#34;.
            
    Returns:
        labels (array): of clustering
    &#34;&#34;&#34;

    if n_clusters &lt; 2 or n_clusters &gt; len(distance):
        print(&#34;n_clusters is either too small or too large, n_clusters must be in range 2 &#34;, len(distance))
        return None

    elif linkage not in [&#34;average&#34;, &#34;complete&#34;, &#34;single&#34;]:
        print(&#34;linkage has to be average, complete or single&#34;)
        return None

    else:

        ag= AgglomerativeClustering(n_clusters = n_clusters, affinity=&#34;precomputed&#34;, distance_threshold=None, linkage=linkage).fit(distance)
        labels = ag.labels_

        return labels</code></pre>
</details>
</dd>
<dt id="graphAlgorithms.clustering.kmedoids_clustering"><code class="name flex">
<span>def <span class="ident">kmedoids_clustering</span></span>(<span>distance, n_clusters=2)</span>
</code></dt>
<dd>
<section class="desc"><p>Kmediods clustering on a provided distance matrix. Based on pyclustering <a href="https://pyclustering.github.io/docs/0.8.2/html/de/d3b/classpyclustering_1_1cluster_1_1optics_1_1optics.html">https://pyclustering.github.io/docs/0.8.2/html/de/d3b/classpyclustering_1_1cluster_1_1optics_1_1optics.html</a></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>distance</code></strong> :&ensp;<code>matrix</code></dt>
<dd>distance matrix to be used for clustering.</dd>
<dt><strong><code>n_clusters</code></strong> :&ensp;<code>int</code></dt>
<dd>amount of clusters that should be found.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>labels</code></strong> :&ensp;<code>array</code></dt>
<dd>of clustering</dd>
<dt><code>created</code> <code>mediods</code> (<code>list</code>): <code>random</code> <code>created</code> <code>mediods</code> <code>used</code> <code>for</code> <code>clustering</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kmedoids_clustering(distance, n_clusters=2):
    &#34;&#34;&#34;
    Kmediods clustering on a provided distance matrix. Based on pyclustering https://pyclustering.github.io/docs/0.8.2/html/de/d3b/classpyclustering_1_1cluster_1_1optics_1_1optics.html
    
    Parameters:
        distance (matrix): distance matrix to be used for clustering.
        n_clusters (int): amount of clusters that should be found.
        
    Returns:
        labels (array): of clustering
        created mediods (list): random created mediods used for clustering
    &#34;&#34;&#34;

    if n_clusters &lt; 2 or n_clusters &gt; len(distance):
        print(&#34;n_clusters is either too small or too large, n_clusters must be in range 2 &#34;, len(distance))
        return None

    

    else:

        #generate random mediods
        initial_medoids = []
    
        for ii in range(n_clusters):
            initial_medoids.append(randrange(len(distance)))

        kmedoids_instance = kmedoids(distance, initial_medoids, data_type=&#39;distance_matrix&#39;)
        # run cluster analysis and obtain results
        kmedoids_instance.process()
        clusters = kmedoids_instance.get_clusters()

        empty = __generate_empty__(len(distance))

        labels = convert_clusters(clusters, empty)


        return np.array(labels),  initial_medoids</code></pre>
</details>
</dd>
<dt id="graphAlgorithms.clustering.multiobjective"><code class="name flex">
<span>def <span class="ident">multiobjective</span></span>(<span>X, labels, min_number_clusters=2, max_number_clusters=None, min_cluster_size=10, max_cluster_size=None, local=True, bet=True, e=0.5, s=0.5, cluster_size_distribution=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Multi objective function to evaluate clusterings. </p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>matrix</code></dt>
<dd>distance matrix, as used for clustering.</dd>
<dt><strong><code>labels</code></strong> :&ensp;<code>list</code></dt>
<dd>list of predicted labels. Needs to be in the same order as X.</dd>
<dt><strong><code>min_number_clusters</code></strong> :&ensp;<code>int</code> or <code>None</code></dt>
<dd>minimum number of allowed clusters, if less than a penalty is applied. If None will not be taken into account. </dd>
<dt><strong><code>max_number_clusters</code></strong> :&ensp;<code>int</code> or <code>None</code></dt>
<dd>maximum number of clusters, if more then penalty is applied. If None will not be taken into account.</dd>
<dt><strong><code>min_cluster_size</code></strong> :&ensp;<code>int</code> or <code>None</code></dt>
<dd>for each cluster with less than x items a penalty is applied. If None will not be taken into account .</dd>
<dt><strong><code>max_cluster_size</code></strong> :&ensp;<code>int</code> or <code>None</code></dt>
<dd>for each cluster with more than x items a penalty is applied. If None will not be taken into account.</dd>
<dt><strong><code>local</code></strong> :&ensp;<code>boolean</code></dt>
<dd>if is True then objective aims at minimizing within cluster similarity based on thr data provided in X. If is False will be ignored.</dd>
<dt><strong><code>bet</code></strong> :&ensp;<code>boolean</code></dt>
<dd>if True objective aims at maximizing dissimilarity between clusters. If False will be ignored.</dd>
<dt><strong><code>e</code></strong> :&ensp;<code>float</code></dt>
<dd>in [0,1]. For each cluster with a mean similarity less than e an additional penalty is applied. If None will be ignored.</dd>
<dt><strong><code>s</code></strong> :&ensp;<code>float</code></dt>
<dd>in [0,1]. Between each cluster pair where thr distance is less than s an additional penalty is applied. If None will be ignored.</dd>
<dt><strong><code>cluster_size_distribution</code></strong> :&ensp;<code>boolean</code></dt>
<dd>if True mean difference of cluster size for each cluster to "most equal" partitioning is applied. Most equal partitioning is len(labels) / number of clusters. If False will be ignored.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>clustering score (float): the closer to 0 the better the clustering is with regards to the selected objectives.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def multiobjective(X, labels, min_number_clusters=2, max_number_clusters=None, min_cluster_size = 10, max_cluster_size=None, local =True, bet=True, e=0.5, s=0.5, cluster_size_distribution = True):
    
    &#34;&#34;&#34;
        Multi objective function to evaluate clusterings. 
        
        Parameters:
            X (matrix): distance matrix, as used for clustering.
            labels (list): list of predicted labels. Needs to be in the same order as X.
            min_number_clusters (int or None): minimum number of allowed clusters, if less than a penalty is applied. If None will not be taken into account. 
            max_number_clusters (int or None): maximum number of clusters, if more then penalty is applied. If None will not be taken into account.
            min_cluster_size (int or None): for each cluster with less than x items a penalty is applied. If None will not be taken into account .
            max_cluster_size (int or None): for each cluster with more than x items a penalty is applied. If None will not be taken into account.
            local (boolean): if is True then objective aims at minimizing within cluster similarity based on thr data provided in X. If is False will be ignored.
            bet (boolean): if True objective aims at maximizing dissimilarity between clusters. If False will be ignored.
            e (float): in [0,1]. For each cluster with a mean similarity less than e an additional penalty is applied. If None will be ignored.
            s (float): in [0,1]. Between each cluster pair where thr distance is less than s an additional penalty is applied. If None will be ignored.
            cluster_size_distribution (boolean): if True mean difference of cluster size for each cluster to &#34;most equal&#34; partitioning is applied. Most equal partitioning is len(labels) / number of clusters. If False will be ignored.
            
        Returns:
            clustering score (float): the closer to 0 the better the clustering is with regards to the selected objectives.            
            
    &#34;&#34;&#34;
    
    
    obj = 0
    
    clus = Counter(labels)
    
    nr_clusters = max(labels)+1
    
    #transform labels into list of lists for each cluster one with index of items
    
    ll = []
    
    for r in range(nr_clusters):
        indices = [i for i, x in enumerate(labels) if x == r]
        ll.append(indices)
        
    #print(ll)
    if min_number_clusters is not None:
        if min_number_clusters &gt; nr_clusters:
            
            obj = obj + min_number_clusters/nr_clusters
            
            #print(&#34;min number of clusters penalty is&#34;,  min_number_clusters/nr_clusters)
            
    if max_number_clusters is not None:
        if max_number_clusters &gt; nr_clusters:
            obj = obj +  max_number_clusters/nr_clusters
            
            #print(&#34;max number of clusters penalty is&#34;, max_number_clusters/nr_clusters)
            
            
    if min_cluster_size is not None:
        t = sum(i &lt; min_cluster_size for i in list(clus.values()))
        obj = obj + (t/nr_clusters)
        
        #print(&#34;min_cluster_size penalty is&#34;, t/nr_clusters)
        
        
    if max_cluster_size is not None:
        t = sum(i &gt; max_cluster_size for i in list(clus.values()))
        obj = obj + (t/nr_clusters)
                  
        #print(&#34;max_cluster_size penalty is&#34;, t/nr_clusters)

    if cluster_size_distribution:
        m = len(labels) / nr_clusters
        t = []
        cl = Counter(labels)

        for i in cl.values():
            t.append(abs(i/m))

        t = statistics.mean(t)

        obj = obj + t


        
        
    if local:
        #get mean cluster similarity scores
        m = []
        pen = 0
        
        for c in ll:
            temp = []
            if len(c) &gt; 1:
                pairs = list(itertools.combinations(c, 2))

                for p in pairs:
                    temp.append(1-X[p[0]][p[1]])


            else:
                temp.append(0)
                
            #print(&#34;local&#34;, temp)
            if e is not None:
                if statistics.mean(temp) &lt; e:
                    pen = pen + 1
            m.append(statistics.mean(temp))

        if len(m) &gt; 1:
            obj = obj + (1-statistics.mean(m))
                  
            #print(&#34;local penalty is&#34;, (1-statistics.mean(m)))
        elif len(m) &gt; 0:
            obj = obj + 1-m[0]
                  
                  
            #print(&#34;local penalty is&#34;, 1-m[0])
            
        obj = obj + (pen/nr_clusters)
                  
                  
        
        
        
        
        
    if bet:
        #get mean dissimilarity between all clusters 
        #this is calculated on an item by item base
        
        m = []
        pen = 0
        
        for cc in list(itertools.combinations(range(nr_clusters),2)):
            c1 = ll[cc[0]]
            c2 = ll[cc[1]]
            
            temp = []
            for i1 in c1:
                for i2 in c2:
                    temp.append(X[i1][i2])
                    
            #print(&#34;bet&#34;, temp)
            if len(temp) &gt; 0:
                m.append(statistics.mean(temp))
                if s is not None:
                    if statistics.mean(temp) &lt; s:
                        pen = pen +1
                        
                        
            
            elif len(temp) &gt; 0:
                m.append(temp[0])
                
                if s is not None:
                    if temp[0] &lt; s:
                        pen = pen +1
                    
                    
        if len(m) &gt; 1:
            obj = obj + (1-statistics.mean(m))
                  
            #print(&#34;bet penalty is&#34;, 1-statistics.mean(m))
        elif len(m) &gt; 0:
            obj = obj + (1-m[0])
                  
            #print(&#34;bet penalty is&#34;, (1-m[0]))
            
            
        if s is not None:
            obj = obj + (pen/len(list(itertools.combinations(range(nr_clusters),2))))

            #print(&#34;bet penalty s is&#34;, (pen/len(list(itertools.combinations(range(nr_clusters),2)))))
        
        
        
        
        
    return obj</code></pre>
</details>
</dd>
<dt id="graphAlgorithms.clustering.optics_clustering"><code class="name flex">
<span>def <span class="ident">optics_clustering</span></span>(<span>distance, radius=2, neighbors=2, n_clusters=2)</span>
</code></dt>
<dd>
<section class="desc"><p>Optics clustering on a provided distance matrix. Based on pyclustering <a href="https://pyclustering.github.io/docs/0.8.2/html/de/d3b/classpyclustering_1_1cluster_1_1optics_1_1optics.html">https://pyclustering.github.io/docs/0.8.2/html/de/d3b/classpyclustering_1_1cluster_1_1optics_1_1optics.html</a></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>distance</code></strong> :&ensp;<code>matrix</code></dt>
<dd>distance matrix to be used for clustering.</dd>
<dt><strong><code>radius</code></strong> :&ensp;<code>int</code></dt>
<dd>connectivity radius. </dd>
<dt><strong><code>neighbors</code></strong> :&ensp;<code>int</code></dt>
<dd>in [1, number of samples -1]</dd>
<dt><strong><code>n_clusters</code></strong> :&ensp;<code>int</code></dt>
<dd>amount of clusters that should be found.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>labels</code></strong> :&ensp;<code>array</code></dt>
<dd>of clustering</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def optics_clustering(distance, radius=2, neighbors=2, n_clusters=2):
    &#34;&#34;&#34;
    Optics clustering on a provided distance matrix. Based on pyclustering https://pyclustering.github.io/docs/0.8.2/html/de/d3b/classpyclustering_1_1cluster_1_1optics_1_1optics.html
    
    Parameters:
        distance (matrix): distance matrix to be used for clustering.
        radius (int): connectivity radius. 
        neighbors (int): in [1, number of samples -1]
        n_clusters (int): amount of clusters that should be found.

    Returns:
            labels (array): of clustering
    &#34;&#34;&#34;

    if n_clusters &lt; 2 or n_clusters &gt; len(distance):
        print(&#34;n_clusters is either too small or too large, n_clusters must be in range 2 &#34;, len(distance))
        return None

    elif neighbors &lt; 1 or neighbors &gt; len(distance)-1:
        print(&#34;neighbors has to be in range 1&#34;, len(distance)-1)
        return None

    else:

        optics_instance = optics(distance, radius, neighbors, n_clusters, data_type=&#34;distance_matrix&#34;)
        # Performs cluster analysis
        optics_instance.process()
        # Obtain results of clustering
        clusters = optics_instance.get_clusters()

        #converts output into labeled array
        empty = __generate_empty__(len(distance))
        

        labels = convert_clusters(clusters, empty)

        return labels</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="graphAlgorithms" href="index.html">graphAlgorithms</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="graphAlgorithms.clustering.affinityPropagation_clustering" href="#graphAlgorithms.clustering.affinityPropagation_clustering">affinityPropagation_clustering</a></code></li>
<li><code><a title="graphAlgorithms.clustering.consensus_clustering" href="#graphAlgorithms.clustering.consensus_clustering">consensus_clustering</a></code></li>
<li><code><a title="graphAlgorithms.clustering.convert_clusters" href="#graphAlgorithms.clustering.convert_clusters">convert_clusters</a></code></li>
<li><code><a title="graphAlgorithms.clustering.create_mean_distance_matrix" href="#graphAlgorithms.clustering.create_mean_distance_matrix">create_mean_distance_matrix</a></code></li>
<li><code><a title="graphAlgorithms.clustering.create_median_distance_matrix" href="#graphAlgorithms.clustering.create_median_distance_matrix">create_median_distance_matrix</a></code></li>
<li><code><a title="graphAlgorithms.clustering.hierarchical_clustering" href="#graphAlgorithms.clustering.hierarchical_clustering">hierarchical_clustering</a></code></li>
<li><code><a title="graphAlgorithms.clustering.kmedoids_clustering" href="#graphAlgorithms.clustering.kmedoids_clustering">kmedoids_clustering</a></code></li>
<li><code><a title="graphAlgorithms.clustering.multiobjective" href="#graphAlgorithms.clustering.multiobjective">multiobjective</a></code></li>
<li><code><a title="graphAlgorithms.clustering.optics_clustering" href="#graphAlgorithms.clustering.optics_clustering">optics_clustering</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.5</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>